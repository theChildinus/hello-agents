# my_simple_agent.py
from typing import Optional
from hello_agents import SimpleAgent, HelloAgentsLLM, Config, Message

class MySimpleAgent(SimpleAgent):
    """
    é‡å†™çš„ç®€å•å¯¹è¯Agent
    å±•ç¤ºå¦‚ä½•åŸºäºæ¡†æ¶åŸºç±»æ„å»ºè‡ªå®šä¹‰Agent
    """

    def __init__(
        self,
        name: str,
        llm: HelloAgentsLLM,
        system_prompt: Optional[str] = None,
        config: Optional[Config] = None
    ):
        super().__init__(name, llm, system_prompt, config)
        print(f"âœ… {name} åˆå§‹åŒ–å®Œæˆï¼ŒåŸºäºæ¡†æ¶åŸºç±»æ„å»º")

    def run(self, input_text: str, **kwargs) -> str:
        """
        é‡å†™çš„è¿è¡Œæ–¹æ³• - å®ç°ç®€å•å¯¹è¯é€»è¾‘
        """
        print(f"ğŸ¤– {self.name} æ­£åœ¨å¤„ç†: {input_text}")

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []

        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        if self.system_prompt:
            messages.append({"role": "system", "content": self.system_prompt})

        # æ·»åŠ å†å²æ¶ˆæ¯
        for msg in self._history:
            messages.append({"role": msg.role, "content": msg.content})

        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append({"role": "user", "content": input_text})

        # è°ƒç”¨LLM
        response = self.llm.invoke(messages, **kwargs)

        # ä¿å­˜åˆ°å†å²è®°å½•
        self.add_message(Message(input_text, "user"))
        self.add_message(Message(response, "assistant"))

        print(f"âœ… {self.name} å“åº”å®Œæˆ")
        return response
    
    def stream_run(self, input_text: str, **kwargs):
        """
        è‡ªå®šä¹‰çš„æµå¼è¿è¡Œæ–¹æ³•
        """
        print(f"ğŸŒŠ {self.name} å¼€å§‹æµå¼å¤„ç†: {input_text}")

        messages = []

        if self.system_prompt:
            messages.append({"role": "system", "content": self.system_prompt})

        for msg in self._history:
            messages.append({"role": msg.role, "content": msg.content})

        messages.append({"role": "user", "content": input_text})

        # æµå¼è°ƒç”¨LLM
        full_response = ""
        print("ğŸ“ å®æ—¶å“åº”: ", end="")
        for chunk in self.llm.stream_invoke(messages, **kwargs):
            full_response += chunk
            print(chunk, end="", flush=True)
            yield chunk

        print()  # æ¢è¡Œ

        # ä¿å­˜å®Œæ•´å¯¹è¯åˆ°å†å²è®°å½•
        self.add_message(Message(input_text, "user"))
        self.add_message(Message(full_response, "assistant"))
        print(f"âœ… {self.name} æµå¼å“åº”å®Œæˆ")